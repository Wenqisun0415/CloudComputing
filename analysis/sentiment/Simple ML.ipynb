{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required packages / modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/peiyuns/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/peiyuns/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"twitter_samples\")\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read positive and negative tweets\n",
    "positive_tweets = twitter_samples.tokenized(\"positive_tweets.json\")\n",
    "negative_tweets = twitter_samples.tokenized(\"negative_tweets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "vec = DictVectorizer()  # Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweets):\n",
    "    new_tweets = []\n",
    "    for tweet in tweets:\n",
    "        new_tweet = []\n",
    "        for word in tweet:\n",
    "            \n",
    "            if word[0] == \"#\":\n",
    "                i = 1\n",
    "                if i == len(word):\n",
    "                    continue\n",
    "                while(word[i] == \"#\"):\n",
    "                      i += 1\n",
    "                if \"#\" in word[i:]:\n",
    "                      continue\n",
    "                new_tweet.append(\"#\" + word[i:])\n",
    "                \n",
    "            if not (word in stop_words or re.search(\"[^a-zA-Z]\",word)):  # if not stopwords and non-alphabetic\n",
    "                new_tweet.append(word)\n",
    "        new_tweets.append(new_tweet)\n",
    "    \n",
    "    # split into training, development, and testing\n",
    "    training, develop_test = train_test_split(new_tweets, test_size=0.2)\n",
    "    development, testing = train_test_split(develop_test, test_size=0.5)   \n",
    "    \n",
    "    return (training, development, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Excluding hashtags containing stop words and non-alphabetic chars\n",
    "(positive_training, positive_develop, positive_test)  = process_tweets(positive_tweets)\n",
    "(negative_training, negative_develop, negative_test)  = process_tweets(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get array of dictionary according to the list of tweets \n",
    "def getDict(tweets):\n",
    "    dict_arr = []\n",
    "    for tweet in tweets:\n",
    "        tweet_dict = {}\n",
    "        for word in tweet:\n",
    "            if word in tweet_dict:\n",
    "                tweet_dict[word] += 1\n",
    "            else:\n",
    "                tweet_dict[word] = 1\n",
    "        dict_arr.append(tweet_dict)\n",
    "    return dict_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, development, testing sets and targets\n",
    "X_train = vec.fit_transform(getDict(positive_training + negative_training))\n",
    "y_train = len(positive_training)*[1] + len(negative_training)*[0]\n",
    "\n",
    "X_dev = vec.transform(getDict(positive_develop + negative_develop))\n",
    "y_dev = len(positive_develop)*[1] + len(negative_develop)*[0]\n",
    "\n",
    "X_test = vec.transform(getDict(positive_test + negative_test))\n",
    "y_test = len(positive_test)*[1] + len(negative_test)*[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes hyperparameters:\n",
      "alpha =   0.001, Score = 0.7240\n",
      "alpha =   0.010, Score = 0.7260\n",
      "alpha =   0.100, Score = 0.7330\n",
      "alpha =   1.000, Score = 0.7370\n",
      "alpha =  10.000, Score = 0.7360\n",
      "alpha = 100.000, Score = 0.7180\n",
      "\n",
      "Logistic Regression hyperparameters\n",
      "penalty = l1, C =    0.001, Score = 0.5000\n",
      "penalty = l2, C =    0.001, Score = 0.6010\n",
      "penalty = l1, C =    0.010, Score = 0.5500\n",
      "penalty = l2, C =    0.010, Score = 0.6890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peiyuns/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty = l1, C =    0.100, Score = 0.6670\n",
      "penalty = l2, C =    0.100, Score = 0.7330\n",
      "penalty = l1, C =    1.000, Score = 0.7510\n",
      "penalty = l2, C =    1.000, Score = 0.7460\n",
      "penalty = l1, C =   10.000, Score = 0.7480\n",
      "penalty = l2, C =   10.000, Score = 0.7500\n",
      "penalty = l1, C =  100.000, Score = 0.7310\n",
      "penalty = l2, C =  100.000, Score = 0.7300\n",
      "penalty = l1, C = 1000.000, Score = 0.7110\n",
      "penalty = l2, C = 1000.000, Score = 0.7210\n",
      "\n",
      "Naive Bayes: best parameter: {'alpha': 1}, score = 0.737000\n",
      "Logistic Regression: best parameter: {'C': 1, 'penalty': 'l1'}, score = 0.751000\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter sets\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "penaltys = ['l1', 'l2']\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "\n",
    "# Init best hyperparameters\n",
    "best_bayes_params = {}\n",
    "best_bayes_score = 0\n",
    "best_logistic_params = {}\n",
    "best_logistic_score = 0\n",
    "\n",
    "print(\"Naive Bayes hyperparameters:\")\n",
    "# Tune naive bayes hyperparameters\n",
    "for alpha in alphas:\n",
    "    bayes = MultinomialNB(alpha = alpha) # Create classifier\n",
    "    bayes.fit(X_train, y_train) # Train model\n",
    "    score = bayes.score(X_dev, y_dev) # Calculate score \n",
    "    print (\"alpha = %7.3f, Score = %.4f\" % (alpha, score)) # Print result\n",
    "    \n",
    "    # Check if better\n",
    "    if score > best_bayes_score:\n",
    "        best_bayes_params = {'alpha':alpha}\n",
    "        best_bayes_score = score\n",
    "\n",
    "print(\"\\nLogistic Regression hyperparameters\")\n",
    "# Tune logistic regression hyperparameters\n",
    "for C in Cs:\n",
    "    for penalty in penaltys:\n",
    "        logistic = LogisticRegression(C = C, penalty = penalty) # Create classifier\n",
    "        logistic.fit(X_train, y_train) # Train model\n",
    "        score = logistic.score(X_dev, y_dev) # Calculate score  \n",
    "        print(\"penalty = %s, C = %8.3f, Score = %.4f\" % (penalty, C, score))\n",
    "    \n",
    "        # Check if better\n",
    "        if score > best_logistic_score:\n",
    "            best_logistic_params = {'C':C, 'penalty':penalty}\n",
    "            best_logistic_score = score\n",
    "            \n",
    "# print(\"\\SVC hyperparameters\")\n",
    "# # Tune SCV hyperparameters\n",
    "# for C in Cs:\n",
    "#     for kernel in kernels:\n",
    "#         svc = SVC(C = C, kernel = kernel) # Create classifier\n",
    "#         svc.fit(X_train, y_train) # Train model\n",
    "#         score = svc.score(X_dev, y_dev) # Calculate score  \n",
    "#         print(\"kernal = %s, C = %8.3f, Score = %.4f\" % (kernel, C, score))\n",
    "    \n",
    "#         # Check if better\n",
    "#         if score > best_logistic_score:\n",
    "#             best_svc_params = {'C':C, 'kernel':kernel}\n",
    "#             best_svc_score = score\n",
    "        \n",
    "print(\"\\nNaive Bayes: best parameter: %s, score = %f\" % (str(best_bayes_params), best_bayes_score))\n",
    "print(\"Logistic Regression: best parameter: %s, score = %f\" % (str(best_logistic_params), best_logistic_score))\n",
    "# print(\"SVC: best parameter: %s, score = %f\" % (str(best_svc_params), best_svc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: f-score = 0.7615, accuracy = 0.7620\n",
      "Logistic Regression: f-score = 0.7230, accuracy = 0.7250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peiyuns/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Classifiers\n",
    "bayes_clf = MultinomialNB(alpha = best_bayes_params['alpha'])\n",
    "logistic_clf = LogisticRegression(C = best_logistic_params['C'], penalty = best_logistic_params['penalty'])\n",
    "# svc_clf = SVC(C = best_svc_params[\"C\"], kernel = best_svc_params[\"kernel\"])\n",
    "\n",
    "\n",
    "# Train classifiers\n",
    "bayes_clf.fit(X_train, y_train)\n",
    "logistic_clf.fit(X_train, y_train)\n",
    "# svc_clf.fit(X_train, y_train)\n",
    "\n",
    "print (\"Naive Bayes: f-score = %.4f, accuracy = %.4f\" % (f1_score(bayes_clf.predict(X_test),y_test, average = 'macro'), accuracy_score(bayes_clf.predict(X_test),y_test)))\n",
    "print (\"Logistic Regression: f-score = %.4f, accuracy = %.4f\" % (f1_score(logistic_clf.predict(X_test),y_test, average = 'macro'), accuracy_score(logistic_clf.predict(X_test),y_test)))\n",
    "# print (\"SVC: f-score = %.4f, accuracy = %.4f\" % (f1_score(bayes_clf.predict(X_test),y_test, average = 'macro'), accuracy_score(bayes_clf.predict(X_test),y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: f-score = 0.6817, accuracy = 0.6830\n",
      "Decision Tree: f-score = 0.6975, accuracy = 0.6990\n"
     ]
    }
   ],
   "source": [
    "criterions = [\"gini\", \"entropy\"]\n",
    "\n",
    "for criterion in criterions:\n",
    "# Classifiers\n",
    "    dt_clf = DecisionTreeClassifier(criterion = criterion)\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    print (\"Decision Tree: f-score = %.4f, accuracy = %.4f\" % (f1_score(dt_clf.predict(X_test),y_test, average = 'macro'), accuracy_score(dt_clf.predict(X_test),y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the classifier and vectorizer\n",
    "pickle.dump(logistic_clf, open(\"Logistic_{C0.1}_{l1}.pk1\", \"wb\"))  \n",
    "pickle.dump(vec, open(\"DictVectorizer.pk1\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the classifier and vectorizer\n",
    "logistic_clf = pickle.load(open(\"Logistic_{C0.1}_{l1}.pk1\", \"rb\"))  \n",
    "vec = pickle.load(open(\"DictVectorizer.pk1\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
